{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "writer scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5Z1_dvqA2U6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rwCVUJbA497"
      },
      "source": [
        "save_path = 'writers_with_img.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WtXKf059Bri"
      },
      "source": [
        "!pip install deep-translator\n",
        "\n",
        "import requests\n",
        "source = 'https://literature.britishcouncil.org'\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from deep_translator import GoogleTranslator\n",
        "from bs4 import NavigableString, Tag\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GFAtp2Qjv3u"
      },
      "source": [
        "writers_df = pd.DataFrame(columns=['name', 'genres', 'birthplace', 'publishers', 'bio', 'critical_perspective', 'work', 'awards', 'image'])\n",
        "\n",
        "for x in range(38):\n",
        "  page = requests.get(source+'/writers/?start='+str(20*i))\n",
        "  main_soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  writers = main_soup.find('div', class_='list-items -with-pagination').find_all('div', class_='list-item')\n",
        "\n",
        "  for i in range (20):\n",
        "    try:\n",
        "      td = writers[i]\n",
        "\n",
        "      url = td.find('a')['href']\n",
        "      writer_page = requests.get(source+url)\n",
        "      writer_soup = BeautifulSoup(writer_page.content, 'html.parser')\n",
        "\n",
        "      name = writer_soup.find('h1', itemprop='name').text\n",
        "      name_sin = GoogleTranslator(source='en', target='si').translate(name)\n",
        "      # print (name, name_sin)\n",
        "\n",
        "      if re.search('[a-zA-Z]', name_sin):\n",
        "        print (name_sin)\n",
        "        continue\n",
        "\n",
        "      try:\n",
        "        img = source+writer_soup.find('div', class_='col-xs-4 artist-img').find('img')['src']\n",
        "      except:\n",
        "        img = 'https://visualpharm.com/assets/128/User-595b40b85ba036ed117dc943.svg'\n",
        "      \n",
        "      genre_tags = writer_soup.find('ul', class_='artist-info__tags').find_all('li')\n",
        "      genres = [i.text for i in genre_tags]\n",
        "      genres_sin = ', '.join([GoogleTranslator(source='en', target='si').translate(i) for i in genres])\n",
        "\n",
        "      birthplace = writer_soup.find('span', itemprop='birthPlace').text\n",
        "      birthplace_sin = GoogleTranslator(source='en', target='si').translate(birthplace)\n",
        "      \n",
        "      publisher_tags = writer_soup.find_all('span', itemprop='worksFor')\n",
        "      publishers_sin = ', '.join([GoogleTranslator(source='en', target='si').translate(i.text) for i in publisher_tags])\n",
        "\n",
        "      bio_tags = writer_soup.find('div', itemprop='description').find_all('p')\n",
        "      bio_sin = ' '.join([GoogleTranslator(source='en', target='si').translate(i.text) for i in bio_tags])\n",
        "\n",
        "      crit_tags = writer_soup.find_all('div', class_='artist-page-waypoint')[0].find('div', class_='typ').find_all('p')\n",
        "      crit_sin = ' '.join([GoogleTranslator(source='en', target='si').translate(i.text) for i in crit_tags])\n",
        "\n",
        "      work_tags = writer_soup.find_all('div', class_='artist-page-waypoint')[1].find_all('div', class_='row')\n",
        "      work_sin = ', '.join([GoogleTranslator(source='en', target='si').translate(i.find('div', itemprop='name').text)+' ('+i.find('div', itemprop='datePublished').text+')' for i in work_tags]) \n",
        "\n",
        "      award_tags = writer_soup.find_all('div', class_='artist-page-waypoint')[2].find_all('div', class_='row')\n",
        "      awards_sin = ', '.join([GoogleTranslator(source='en', target='si').translate(i.find('div', class_='col-md-10').text)+' ('+i.find('div', class_='col-md-2').text+')' for i in award_tags]) \n",
        "\n",
        "      writers_df.loc[20*x+i] = [name_sin, genres_sin, birthplace_sin, publishers_sin, bio_sin, crit_sin, work_sin, awards_sin, img]\n",
        "      print (x, i, 20*x+i, name, name_sin)\n",
        "\n",
        "    except:\n",
        "      print ('##Error', name)\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJp9w-dpNz67"
      },
      "source": [
        "writers_df.to_csv(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}